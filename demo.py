"""
Thai Energy RAG System - Perfect for PEA Demo Tomorrow!
Fetches content dynamically from URLs using your scraped metadata
"""

import pandas as pd
import gradio as gr
import requests
from bs4 import BeautifulSoup
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re
import json
from typing import List, Dict, Tuple
import time
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ThaiEnergyRAG:
    def __init__(self, csv_file: str):
        print("üöÄ Loading Thai Energy RAG System...")
        print("=" * 60)
        
        # Load metadata
        self.df = pd.read_csv(csv_file, encoding='utf-8')
        print(f"üìä Loaded {len(self.df)} documents")
        
        # Initialize embedding model
        print("ü§ñ Loading multilingual embedding model...")
        self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
        # Prepare document metadata
        self.documents = self.prepare_documents()
        
        # Create embeddings
        print("üîÑ Creating metadata embeddings...")
        self.embeddings = self.model.encode(self.documents)
        
        # Cache for fetched content
        self.content_cache = {}
        
        print("‚úÖ Thai Energy RAG System Ready!")
        print("üéØ Perfect for PEA Demo Tomorrow!")
        print("=" * 60)
    
    def prepare_documents(self) -> List[str]:
        documents = []
        for _, row in self.df.iterrows():
            title = str(row.get('Document_Title_Thai', 'Unknown'))
            source = str(row.get('Source', 'Unknown'))
            doc_type = str(row.get('Document_Type', 'Document'))
            doc_text = f"{source}: {title} ({doc_type})"
            documents.append(doc_text)
        return documents
    
    def fetch_content_from_url(self, url: str) -> str:
        if url in self.content_cache:
            return self.content_cache[url]
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            if 'text/html' in response.headers.get('content-type', ''):
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Remove scripts and styles
                for script in soup(["script", "style"]):
                    script.decompose()
                
                text = soup.get_text()
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                if len(text) > 2000:
                    text = text[:2000] + "..."
                
                self.content_cache[url] = text
                return text
            else:
                return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ"
                
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")
            return f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å URL ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ"
    
    def search_documents(self, query: str, top_k: int = 3) -> List[Dict]:
        query_embedding = self.model.encode([query])
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            row_data = self.df.iloc[idx]
            results.append({
                'index': idx,
                'similarity': similarities[idx],
                'title': row_data.get('Document_Title_Thai', 'Unknown'),
                'source': row_data.get('Source', 'Unknown'),
                'doc_type': row_data.get('Document_Type', 'Document'),
                'url': row_data.get('Document_URL', ''),
                'metadata': self.documents[idx]
            })
        
        return results
    
    def answer_question(self, question: str) -> str:
        print(f"üîç Processing question: {question}")
        
        results = self.search_documents(question, top_k=3)
        
        if not results or results[0]['similarity'] < 0.1:
            return self.create_fallback_response(question)
        
        response_parts = []
        response_parts.append("‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÑ‡∏ó‡∏¢:\n")
        
        for i, result in enumerate(results[:2], 1):
            title = result['title']
            source = result['source']
            doc_type = result['doc_type']
            url = result['url']
            similarity = result['similarity']
            
            response_parts.append(f"\nüìÑ ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà {i}: {source}")
            response_parts.append(f"üìã ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á: {title}")
            response_parts.append(f"üìù ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {doc_type}")
            
            if url and url != 'nan':
                print(f"üì° Fetching content from: {url}")
                content = self.fetch_content_from_url(url)
                if content and len(content) > 50:
                    response_parts.append(f"üìñ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {content[:300]}...")
                else:
                    response_parts.append("üìñ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ")
            
            response_parts.append(f"üéØ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {similarity:.1%}")
            response_parts.append(f"üîó ‡∏•‡∏¥‡∏á‡∏Å‡πå: {url}\n")
        
        response_parts.append("\nüí° ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á")
        
        return "\n".join(response_parts)
    
    def create_fallback_response(self, question: str) -> str:
        question_lower = question.lower()
        
        if 'pea' in question_lower or '‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ' in question_lower:
            pea_docs = self.df[self.df['Source'] == 'PEA']
            if len(pea_docs) > 0:
                sample_doc = pea_docs.iloc[0]
                return f"""üîç ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö PEA:

üìÑ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: {sample_doc.get('Document_Title_Thai', 'Unknown')}
üè¢ ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô: ‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ (PEA)
üìù ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {sample_doc.get('Document_Type', 'Document')}

üí° ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô:
- PEA ‡∏°‡∏µ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏´‡∏°‡∏∏‡∏ô‡πÄ‡∏ß‡∏µ‡∏¢‡∏ô?
- ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏à‡∏≤‡∏Å PEA?
- ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á PEA?"""

        elif 'mea' in question_lower or '‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏´‡∏•‡∏ß‡∏á' in question_lower:
            mea_docs = self.df[self.df['Source'] == 'MEA']
            if len(mea_docs) > 0:
                sample_doc = mea_docs.iloc[0]
                return f"""üîç ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö MEA:

üìÑ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: {sample_doc.get('Document_Title_Thai', 'Unknown')}
üè¢ ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô: ‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏´‡∏•‡∏ß‡∏á (MEA)
üìù ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: {sample_doc.get('Document_Type', 'Document')}

üí° ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô:
- MEA ‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?
- ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á MEA ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢?
- ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏à‡∏≤‡∏Å MEA?"""
        
        return """üîç ‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

üìö ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å:
‚Ä¢ ‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ (PEA) - 114 ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
‚Ä¢ ‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏ô‡∏Ñ‡∏£‡∏´‡∏•‡∏ß‡∏á (MEA) - 82 ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£  
‚Ä¢ ‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô (ERC) - 69 ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
‚Ä¢ ‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÅ‡∏ú‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô (EPPO) - 44 ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£

üí° ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö:
- ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô
- ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤
- ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
- ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏´‡∏°‡∏∏‡∏ô‡πÄ‡∏ß‡∏µ‡∏¢‡∏ô"""


def create_demo_interface(rag_system):
    def chat_function(message, history):
        try:
            response = rag_system.answer_question(message)
            return response
        except Exception as e:
            return f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}"
    
    demo = gr.ChatInterface(
        fn=chat_function,
        title="üáπüá≠ Thai Energy Knowledge Assistant - PEA Demo",
        description="""
**‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÑ‡∏ó‡∏¢**

‚ú® **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:**
- ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö PEA, MEA, ERC, EPPO
- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á  
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
- ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÅ‡∏•‡∏∞‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£

üöÄ **‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠ PEA ‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ!**
        """,
        examples=[
            "What is PEA's electricity policy?",
            "PEA ‡∏°‡∏µ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏´‡∏≤‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?",
            "Tell me about MEA's electrical safety standards",
            "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á MEA",
            "How does ERC regulate the energy sector?",
            "‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏î‡∏π‡πÅ‡∏•‡∏†‡∏≤‡∏Ñ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á ‡∏Å‡∏Å‡∏û.",
            "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡∏à‡∏≤‡∏Å PEA",
            "MEA ‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?"
        ],
        theme=gr.themes.Soft()
    )
    
    return demo


if __name__ == "__main__":
    CSV_FILE = "Thai_Energy_High_Priority_2025-09-28_12-16.csv"
    
    try:
        print("üáπüá≠ THAI ENERGY RAG SYSTEM")
        print("=" * 60)
        print("üéØ Ready for PEA Demo Tomorrow!")
        print("=" * 60)
        
        # Create RAG system
        rag = ThaiEnergyRAG(CSV_FILE)
        
        # Test the system
        print("\nüß™ Testing the system...")
        test_question = "What does PEA say about electricity distribution?"
        print(f"\n‚ùì Test: {test_question}")
        answer = rag.answer_question(test_question)
        print(f"‚úÖ Response: {answer[:200]}...")
        
        # Create and launch demo
        print(f"\nüöÄ Starting Thai Energy Demo Server...")
        print(f"üåê Perfect for PEA Presentation Tomorrow!")
        
        demo = create_demo_interface(rag)
        
        # Launch with public sharing for demo
        demo.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=True,
            show_error=True
        )
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        print("\nüìã Install required packages:")
        print("pip install -r requirements.txt")
